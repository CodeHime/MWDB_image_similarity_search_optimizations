{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "MWDB_Phase2_task1_2_5.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "pnZBlWQULTJF",
        "KJK56QNmL8uS"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pnZBlWQULTJF"
      },
      "source": [
        "# Load files"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rExTTvs4NGsx"
      },
      "source": [
        "# from numpy.linalg import eig\n",
        "from scipy.linalg import eig\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "def pca_cust(A, k_num=-1, return_order=False):\n",
        "    if k_num == -1:\n",
        "        k_num = min(A.shape[0], A.shape[1])\n",
        "    if k_num > min(A.shape[0], A.shape[1]):\n",
        "        raise ValueError(k_num + \" must be less than min(A.shape[0],A.shape[1]) \" + min(A.shape[0], A.shape[1]))\n",
        "    cov = np.cov(A)\n",
        "    k, U = eig(cov)\n",
        "\n",
        "    U_sorted, k_order = get_sorted_matrix_on_weights(k, U, return_order=True)\n",
        "    k = k[::-1]\n",
        "    U = U.transpose()[::-1]\n",
        "    if return_order:\n",
        "        return U.transpose().astype(float), np.diag(k.astype(float)), U.astype(float), k_order\n",
        "    else:\n",
        "        return U.transpose().astype(float), np.diag(k.astype(float)), U.astype(float)\n",
        "\n",
        "\n",
        "def svd_cust(A, k_num=-1, return_order=False):\n",
        "    if k_num == -1:\n",
        "        k_num = min(A.shape[0], A.shape[1])\n",
        "    if k_num > min(A.shape[0], A.shape[1]):\n",
        "        raise ValueError(k_num + \" must be less than min(A.shape[0],A.shape[1]) \" + min(A.shape[0], A.shape[1]))\n",
        "    transpose = A.shape[0] > A.shape[1]\n",
        "    if transpose:\n",
        "        A = np.transpose(A)\n",
        "    data_mat = np.dot(A, np.transpose(A))\n",
        "    feature_mat = np.dot(np.transpose(A), A)\n",
        "\n",
        "    k1, U = eig(data_mat)\n",
        "    k2, V = eig(feature_mat)\n",
        "\n",
        "    V = np.transpose(V).astype(float)\n",
        "    U_sorted, k1_order = get_sorted_matrix_on_weights(k1, U, return_order=True)\n",
        "    k1 = np.transpose(np.array(U_sorted, dtype=object))[0]\n",
        "    U = np.stack(np.transpose(np.array(U_sorted, dtype=object))[1])\n",
        "\n",
        "    V_sorted = get_sorted_matrix_on_weights(k2, V)\n",
        "    V = []\n",
        "    for i in range(len(V_sorted)):\n",
        "        if i < len(k1):\n",
        "            V.append(V_sorted[i][1] * V_sorted[i][0] / k1[i])\n",
        "        else:\n",
        "            break\n",
        "\n",
        "    if return_order:\n",
        "        if not transpose:\n",
        "            return np.array(U).astype(float), np.diag(np.nan_to_num(np.sqrt(k1.astype(float)))), np.array(V).astype(\n",
        "                float), k1_order\n",
        "        else:\n",
        "            return np.transpose(np.array(V)).astype(float), np.diag(np.nan_to_num(np.sqrt(k1.astype(float)))), np.transpose(\n",
        "                np.array(U).astype(float)), k1_order\n",
        "    else:\n",
        "        if not transpose:\n",
        "            return np.array(U).astype(float), np.diag(np.nan_to_num(np.sqrt(k1.astype(float)))), np.array(V).astype(float)\n",
        "        else:\n",
        "            return np.transpose(np.array(V)).astype(float), np.diag(np.nan_to_num(np.sqrt(k1.astype(float)))), np.transpose(\n",
        "                np.array(U).astype(float))\n",
        "\n",
        "\n",
        "def get_sorted_matrix_on_weights(weights, V, return_order=False):\n",
        "    v_dict = {}\n",
        "    weight_dict = {}\n",
        "    V = np.array(V)\n",
        "    for i in range(len(weights)):\n",
        "        if weights[i] < 0:\n",
        "            v_dict.update({-weights[i]: -V[i]})\n",
        "            weight_dict.update({-weights[i]: i})\n",
        "        else:\n",
        "            v_dict.update({weights[i]: V[i]})\n",
        "            weight_dict.update({weights[i]: i})\n",
        "\n",
        "    V_sorted = sorted(v_dict.items())[::-1]\n",
        "    weights_order = sorted(weight_dict.items())[::-1]\n",
        "    # print(V_sorted)\n",
        "    # print(weights_order)\n",
        "    if return_order:\n",
        "        return V_sorted, weights_order\n",
        "    else:\n",
        "        return V_sorted"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Bd24sV6TK6yg"
      },
      "source": [
        "# Top K image search using original images\n",
        "# Here I am using a library named faiss for the similarity search\n",
        "# Eucleadean and cosine:: TODO :: why this and not other distances\n",
        "import random\n",
        "import os\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from scipy.stats import wasserstein_distance\n",
        "from scipy.spatial import distance\n",
        "\n",
        "\n",
        "def euclidean_fn(xb, xq):\n",
        "  \"\"\"\n",
        "  Calculate the euclidean distance and return distance matrix\n",
        "  :param xb: Data matrix to find similarity in\n",
        "  :param xq:  Query matrix to find similarity for\n",
        "  \"\"\"\n",
        "  eu=np.sqrt(np.sum(np.square(xb-xq), axis=1))\n",
        "  return eu\n",
        "\n",
        "\n",
        "def cosine_fn(xb, xq):\n",
        "  \"\"\"\n",
        "  Calculate the cosine distance and return distance matrix\n",
        "  :param xb: Data matrix to find similarity in\n",
        "  :param xq:  Query matrix to find similarity for\n",
        "  \"\"\"\n",
        "  cos = np.array([distance.cosine(i, xq) for i in xb])\n",
        "  return cos\n",
        "\n",
        "\n",
        "def manhattan_fn(xb, xq):\n",
        "  \"\"\"\n",
        "  Calculate the manhattan distance and return distance matrix\n",
        "  :param xb: Data matrix to find similarity in\n",
        "  :param xq:  Query matrix to find similarity for\n",
        "  \"\"\"\n",
        "  man = np.sum(np.absolute(xb-xq), axis=1)\n",
        "  return man\n",
        "\n",
        "\n",
        "def kl_divergence_fn(xb, xq):\n",
        "  \"\"\"\n",
        "  Calculate the kl divergence and return divergence matrix\n",
        "  :param xb: Data matrix to find similarity in\n",
        "  :param xq:  Query matrix to find similarity for\n",
        "  \"\"\"\n",
        "  return np.sum(np.where(xb != 0, xb * np.log(xb / xq), 0), axis=1)\n",
        "\n",
        "\n",
        "def euclidean(xb, k, xq):\n",
        "  \"\"\"\n",
        "  Calculate the euclidean distance and return the top k values\n",
        "  :param xb: Data matrix to find similarity in\n",
        "  :param k: Number of top objects to return\n",
        "  :param xq:  Query matrix to find similarity for\n",
        "  \"\"\"                       \n",
        "  eu=euclidean_fn(xb, xq)\n",
        "  idx = np.argpartition(eu, k)[:k]\n",
        "  return eu[idx], idx\n",
        "\n",
        "def kl_divergence(xb, k, xq):\n",
        "  \"\"\"\n",
        "  Calculate the kl divergence and return divergence matrix\n",
        "  :param xb: Data matrix to find similarity in\n",
        "  :param k: Number of top objects to return\n",
        "  :param xq:  Query matrix to find similarity for\n",
        "  \"\"\"\n",
        "  kl = kl_divergence_fn(xb, xq)\n",
        "  idx = np.argpartition(kl, k)[:k]\n",
        "  return kl[idx], idx\n",
        "\n",
        "\n",
        "def cosine(xb, k, xq):\n",
        "  \"\"\"\n",
        "  Calculate the cosine distance and return the top k values\n",
        "  :param xb: Data matrix to find similarity in\n",
        "  :param k: Number of top objects to return\n",
        "  :param xq:  Query matrix to find similarity for\n",
        "  \"\"\"\n",
        "  cos = cosine_fn(xb, xq)\n",
        "  idx = np.argpartition(em, k)[:k]\n",
        "  return cos[idx], idx\n",
        "\n",
        "\n",
        "def manhattan(xb, k, xq):\n",
        "  \"\"\"\n",
        "  Calculate the manhattan distance and return the top k values\n",
        "  :param xb: Data matrix to find similarity in\n",
        "  :param k: Number of top objects to return\n",
        "  :param xq:  Query matrix to find similarity for\n",
        "  \"\"\"\n",
        "  man = manhattan_fn(xb, xq)\n",
        "  idx = np.argpartition(man, k)[:k]\n",
        "  return man[idx], idx\n",
        "\n",
        "\n",
        "def earth_movers(xb, k, xq):\n",
        "  \"\"\"\n",
        "  Calculate the earth movers distance and return the top k values\n",
        "  :param xb: Data matrix to find similarity in\n",
        "  :param k: Number of top objects to return\n",
        "  :param xq:  Query matrix to find similarity for\n",
        "  \"\"\"\n",
        "  # em = np.array([wasserstein_distance(i,xq[0]) for i in xb])\n",
        "  em = np.array([wasserstein_distance(np.histogram(i)[1], np.histogram(xq)[1]) for i in xb])\n",
        "  idx = np.argpartition(em, k)[:k]\n",
        "  return em[idx], idx\n",
        "\n",
        "\n",
        "def top_k_match(xb, k, xq, method=\"euclidean\"):\n",
        "  \"\"\"\n",
        "  General function to call distance functions\n",
        "  :param xb: Data matrix to find similarity in\n",
        "  :param k: Number of top objects to return\n",
        "  :param xq:  Query matrix to find similarity for\n",
        "  \"\"\"\n",
        "  if method == \"euclidean\":\n",
        "    return euclidean(xb, k, xq)\n",
        "  elif method == \"cosine\":\n",
        "    return cosine(xb, k, xq)\n",
        "  elif method == \"manhattan\":\n",
        "    return manhattan(xb, k, xq)\n",
        "  elif method == \"earth_movers\":\n",
        "    return earth_movers(xb, k, xq)\n",
        "\n",
        "\n",
        "def get_image_file(features_dir, image_ids):\n",
        "  \"\"\"\n",
        "  Get image-image id mapping file\n",
        "  :param features_dir: features directory where mapping is stored\n",
        "  :param image_ids: image ids to fetch\n",
        "  \"\"\"\n",
        "  df = pd.read_csv(os.path.join(features_dir, \"image_ids.csv\"))#.iloc[image_ids]\n",
        "  return df[\"image_idx\"].to_list()"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2Bybx06CLXq_"
      },
      "source": [
        "import pandas as pd\n",
        "import os\n",
        "\n",
        "\n",
        "class Pca:\n",
        "    \"\"\"\n",
        "    Represents PCA dimension technique\n",
        "    ...\n",
        "    Attributes:\n",
        "        k: int\n",
        "            Number of reduced features\n",
        "\n",
        "        X: ndarray of shape (num_objects, num_features)\n",
        "            Data matrix to be reduced\n",
        "\n",
        "    Methods:\n",
        "        transform(X)\n",
        "            Transforms and returns X in the latent semantic space and the latent semantics\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, *args):\n",
        "        \"\"\"\n",
        "        Parameters:\n",
        "            k: int\n",
        "                Number of reduced features\n",
        "\n",
        "            X: ndarray of shape (num_objects, num_features)\n",
        "                Data matrix to be reduced\n",
        "        \"\"\"\n",
        "        if (len(args)) == 2:\n",
        "            X = args[1]\n",
        "            k = args[0]\n",
        "            self.x_ = np.array(X, dtype=np.float32)\n",
        "            self.features_ = self.x_.shape[1]\n",
        "\n",
        "            self.x_covariance_ = np.cov(self.x_)\n",
        "            self.eigen_values_, self.eigen_vectors_ = eig(self.x_covariance_)\n",
        "\n",
        "            temp, self.sub_wt_pairs = get_sorted_matrix_on_weights(self.eigen_values_, self.eigen_vectors_, return_order=True)\n",
        "            self.eigen_vectors_ = self.eigen_vectors_.transpose()[::-1]\n",
        "            self.eigen_values_ = self.eigen_values_[::-1]\n",
        "\n",
        "            self.u_, self.s_, self.u_transpose_ = self.eigen_vectors_[:k].transpose(), \\\n",
        "                                                  np.diag(self.eigen_values_[:k].astype(np.float)), \\\n",
        "                                                  self.eigen_vectors_[:k]\n",
        "        elif (len(args)) == 1:\n",
        "            self.u_ = pd.read_csv(os.path.join(args[0], \"U.csv\")).to_numpy()\n",
        "            self.u_transpose_ = self.u_.transpose()\n",
        "            self.s_ = pd.read_csv(os.path.join(args[0], \"S.csv\")).to_numpy()\n",
        "            self.x_ = pd.read_csv(os.path.join(args[0], \"X.csv\")).to_numpy()\n",
        "            self.sub_wt_pairs = pd.read_csv(os.path.join(args[0], \"sub_wt_pairs.csv\")).to_numpy()\n",
        "        else:\n",
        "            raise Exception(\"Invalid object instantiation: parameters must be either <data_matrix:2D numpy>,<k:int> \"\n",
        "                            \"or <folder:string>.\")\n",
        "\n",
        "    def get_decomposition(self):\n",
        "        \"\"\"\n",
        "        Parameters:\n",
        "            X: ndarray of shape (num_objects, num_features)\n",
        "                Data matrix to be reduced\n",
        "\n",
        "        Returns:\n",
        "            Transforms and returns X in the latent semantic space and the latent semantics\n",
        "        \"\"\"\n",
        "        return self.u_, self.s_, self.u_transpose_\n",
        "\n",
        "    def get_latent_features(self):\n",
        "        \"\"\"\n",
        "        :return: components of PCA\n",
        "        \"\"\"\n",
        "        return self.u_, self.s_\n",
        "\n",
        "    def transform(self, data_matrix):\n",
        "        \"\"\"\n",
        "        :param data_matrix: matrix to transform (query_objects, num_features)\n",
        "        :return: pca transformed matrix\n",
        "        \"\"\"\n",
        "        Q = np.concatenate((self.x_, data_matrix), axis=0)\n",
        "        return np.dot(np.cov(Q)[-1][:-1], self.u_)\n",
        "\n",
        "    def get_obj_weight_pairs(self):\n",
        "        \"\"\"\n",
        "        :return: objects\n",
        "        :return: weights\n",
        "        \"\"\"\n",
        "        return self.sub_wt_pairs\n",
        "\n",
        "    def save(self, folder):\n",
        "        \"\"\"\n",
        "        Save PCA to given folder\n",
        "        \"\"\"\n",
        "        pd.DataFrame(self.u_).to_csv(os.path.join(folder, \"U.csv\"), index=False)\n",
        "        pd.DataFrame(self.x_).to_csv(os.path.join(folder, \"X.csv\"), index=False)\n",
        "        pd.DataFrame(self.s_).to_csv(os.path.join(folder, \"S.csv\"), index=False)\n",
        "        pd.DataFrame(self.sub_wt_pairs).to_csv(os.path.join(folder, \"sub_wt_pairs.csv\"), index=False)\n",
        "\n",
        "    def get_top_k_matches(self, k, xq):\n",
        "        return euclidean(np.dot(self.u_, np.diag(self.s_)), k, self.transform([xq]))\n"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cuQvt0fyLbEP"
      },
      "source": [
        "from sklearn.decomposition import LatentDirichletAllocation as LDA\n",
        "\n",
        "\n",
        "class Lda:\n",
        "    \"\"\"\n",
        "    Represents LDA feature reduction class\n",
        "    ...\n",
        "    Attributes:\n",
        "\n",
        "        Data_matrix: ndarray of shape (num_objects, num_features)\n",
        "            Data matrix to be reduced\n",
        "\n",
        "        k: int\n",
        "            Number of reduced features\n",
        "\n",
        "    Methods:\n",
        "\n",
        "        compute_lda(X):\n",
        "            Returns a Matrix of K latent features * N objects\n",
        "\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, *args):\n",
        "        \"\"\"\n",
        "        :param data_matrix: input matrix of shape (num_objects, num_features)\n",
        "                Data matrix to be reduced\n",
        "        :param k: Number of reduced features\n",
        "\n",
        "        OR \n",
        "        :param folder: folder containing LDA latent features\n",
        "\n",
        "        \"\"\"\n",
        "        if (len(args)) == 2:\n",
        "            # normal object instantiation with data_matrix and k\n",
        "            self.data_matrix = args[0]\n",
        "            self.k = args[1]\n",
        "            self.lda_ = LDA(n_components=self.k).fit(self.data_matrix)\n",
        "            self.new_object_map = self.transform(self.data_matrix)\n",
        "            # Take average as sum of all probabilities will always be 1\n",
        "            temp, self.sub_wt_pairs = get_sorted_matrix_on_weights(self.data_matrix, np.average(self.lda_.components_, axis=0), return_order=True)\n",
        "        elif (len(args)) == 1:\n",
        "            # load object from folder\n",
        "            self.lda_ = LDA()\n",
        "            self.lda_.components_ = pd.read_csv(os.path.join(args[0], \"components.csv\")).to_numpy()\n",
        "            self.lda_.exp_dirichlet_component_ = pd.read_csv(\n",
        "                os.path.join(args[0], \"exp_dirichlet_components.csv\")).to_numpy()\n",
        "            self.new_object_map = pd.read_csv(os.path.join(args[0], \"new_object_map.csv\")).to_numpy()\n",
        "            self.lda_.set_params(n_components=self.lda_.components_.shape[0])\n",
        "            self.lda_.doc_topic_prior_ = 1 / self.lda_.components_.shape[0]\n",
        "            self.sub_wt_pairs = pd.read_csv(os.path.join(args[0], \"sub_wt_pairs.csv\")).to_numpy()\n",
        "        else:\n",
        "            raise Exception(\"Invalid object instantiation: parameters must be either <data_matrix:2D numpy>,<k:int> \"\n",
        "                            \"or <folder:string>.\")\n",
        "\n",
        "    def transform(self, data_matrix):\n",
        "        \"\"\"\n",
        "        :param data_matrix: matrix to transform (query_objects, num_features)\n",
        "        :return: lda transformed matrix\n",
        "        \"\"\"\n",
        "        return self.lda_.transform(data_matrix)\n",
        "\n",
        "    def get_latent_features(self):\n",
        "        \"\"\"\n",
        "        :return: components of lda(normalized)\n",
        "        \"\"\"\n",
        "        return self.lda_.components_\n",
        "\n",
        "    def get_obj_weight_pairs(self):\n",
        "        \"\"\"\n",
        "        :param data_matrix: matrix to transform (query_objects, num_features)\n",
        "        :return: objects - lda transformed matrix\n",
        "        :return: weights - components of lda(normalized)\n",
        "        \"\"\"\n",
        "        # TODO: HOW?\n",
        "        return self.sub_wt_pairs\n",
        "\n",
        "    def save(self, folder):\n",
        "        \"\"\"\n",
        "        Save LDA to given folder\n",
        "        \"\"\"\n",
        "        pd.DataFrame(self.lda_.components_).to_csv(os.path.join(folder, \"components.csv\"), index=False)\n",
        "        pd.DataFrame(self.lda_.exp_dirichlet_component_).to_csv(os.path.join(folder, \"exp_dirichlet_components.csv\"), index=False)\n",
        "        pd.DataFrame(self.new_object_map).to_csv(os.path.join(folder, \"new_object_map.csv\"), index=False)\n",
        "\n",
        "    def get_top_k_matches(self, k, xq):\n",
        "        # KL divergence as it is a probability distribution\n",
        "        return kl_divergence(self.new_object_map, k, self.transform([xq]))\n"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BP83_nDkLcgR"
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import os\n",
        "\n",
        "\n",
        "class Svd:\n",
        "    \"\"\"\n",
        "    Represents SVD dimension technique\n",
        "    ...\n",
        "    Attributes:\n",
        "        k: int\n",
        "            Number of reduced features\n",
        "\n",
        "        X: ndarray of shape (num_objects, num_features)\n",
        "            Data matrix to be reduced\n",
        "\n",
        "    Methods:\n",
        "        transform(X)\n",
        "            Transforms and returns X in the latent semantic space and the latent semantics\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, *args):\n",
        "        \"\"\"\n",
        "        :param data_matrix: input matrix of shape (num_objects, num_features)\n",
        "                Data matrix to be reduced\n",
        "        :param k: Number of reduced features\n",
        "\n",
        "        OR\n",
        "        :param folder: folder containing LDA latent features\n",
        "\n",
        "        \"\"\"\n",
        "        if (len(args)) == 2:\n",
        "            # normal object instantiation with data_matrix and k\n",
        "            self.U, self.S, self.VT, self.sub_wt_pairs = svd_cust(args[0], k_num=args[1],return_order=True)\n",
        "        elif (len(args)) == 1:\n",
        "            self.U = pd.read_csv(os.path.join(args[0], \"U.csv\")).to_numpy()\n",
        "            self.S = pd.read_csv(os.path.join(args[0], \"S.csv\")).to_numpy()\n",
        "            self.VT = pd.read_csv(os.path.join(args[0], \"VT.csv\")).to_numpy()\n",
        "            self.sub_wt_pairs = pd.read_csv(os.path.join(args[0], \"sub_wt_pairs.csv\")).to_numpy()\n",
        "        else:\n",
        "            raise Exception(\"Invalid object instantiation: parameters must be either <data_matrix:2D numpy>,<k:int> \"\n",
        "                            \"or <folder:string>.\")\n",
        "\n",
        "    def get_decomposition(self):\n",
        "        \"\"\"\n",
        "        Parameters:\n",
        "            X: ndarray of shape (num_objects, num_features)\n",
        "                Data matrix to be reduced\n",
        "\n",
        "        Returns:\n",
        "            Transforms and returns X in the latent semantic space and the latent semantics\n",
        "        \"\"\"\n",
        "        return self.U, self.S, self.VT\n",
        "\n",
        "    def get_latent_features(self):\n",
        "        \"\"\"\n",
        "        :return: U and S\n",
        "        \"\"\"\n",
        "        return self.U, self.S\n",
        "\n",
        "    def transform(self, data_matrix):\n",
        "        \"\"\"\n",
        "        :param data_matrix: matrix to transform (query_objects, num_features)\n",
        "        :return: pca transformed matrix\n",
        "        \"\"\"\n",
        "        return np.dot(data_matrix, np.transpose(self.VT))\n",
        "\n",
        "    def get_obj_weight_pairs(self):\n",
        "        \"\"\"\n",
        "        :return: objects\n",
        "        :return: weights\n",
        "        \"\"\"\n",
        "        return self.sub_wt_pairs\n",
        "\n",
        "    def save(self, folder):\n",
        "        \"\"\"\n",
        "        Save SVD to given folder\n",
        "        \"\"\"\n",
        "        pd.DataFrame(self.U).to_csv(os.path.join(folder, \"U.csv\"), index=False)\n",
        "        pd.DataFrame(self.S).to_csv(os.path.join(folder, \"S.csv\"), index=False)\n",
        "        pd.DataFrame(self.VT).to_csv(os.path.join(folder, \"VT.csv\"), index=False)\n",
        "        pd.DataFrame(self.sub_wt_pairs).to_csv(os.path.join(folder, \"sub_wt_pairs.csv\"), index=False)\n",
        "\n",
        "    def get_top_k_matches(self, k, xq):\n",
        "        return euclidean(np.dot(self.U, self.S), k, self.transform([xq]))\n",
        "\n"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rYfp-1PMLeVa"
      },
      "source": [
        "from sklearn.cluster import KMeans\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import os\n",
        "\n",
        "\n",
        "class Kmeans:\n",
        "    \"\"\"\n",
        "    Represents Kmeans dimension technique\n",
        "    ...\n",
        "    Attributes:\n",
        "        k: int\n",
        "            Number of reduced features\n",
        "\n",
        "        X: ndarray of shape (num_objects, num_features)\n",
        "            Data matrix to be reduced\n",
        "\n",
        "    Methods:\n",
        "        transform(X)\n",
        "            Transforms and returns X in the latent semantic space and the latent semantics\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, *args):\n",
        "        \"\"\"\n",
        "        :param data_matrix: input matrix of shape (num_objects, num_features)\n",
        "                Data matrix to be reduced\n",
        "        :param k: Number of reduced features\n",
        "\n",
        "        OR\n",
        "        :param folder: folder containing LDA latent features\n",
        "\n",
        "        \"\"\"\n",
        "        if (len(args)) == 2:\n",
        "            # normal object instantiation with data_matrix and k\n",
        "            imgs_slc = args[0]\n",
        "            k = args[1]\n",
        "            num_imgs = len(imgs_slc)\n",
        "            arr_shp = imgs_slc[0][1].shape[0]\n",
        "            imgs = np.zeros((num_imgs, arr_shp))\n",
        "            for i in range(0, num_imgs):\n",
        "                imgs[i] = imgs_slc[i][1]\n",
        "            imgs_flat = imgs.reshape(num_imgs, arr_shp)\n",
        "            kmeans = KMeans(n_clusters=k, random_state=0).fit(imgs_flat)\n",
        "            self.centers = kmeans.cluster_centers_\n",
        "            self.new_object_map = np.zeros((num_imgs, k))\n",
        "            self.weight = np.zeros((num_imgs))\n",
        "            # TODO: # features then truncate to k?\n",
        "            # TODO: ask logic for two loops\n",
        "            for i in range(0, num_imgs):\n",
        "                for j in range(0, k):\n",
        "                    self.new_object_map[i][j] = manhattan_fn(imgs_flat[i], self.centers[j])\n",
        "                self.weight[i] = np.sum(self.new_object_map[i][:])\n",
        "            # Since good latent semantics give high discrimination power\n",
        "            # INFO: variance or distance maximized? Distance as we have a center not a line or curve\n",
        "            temp, self.sub_wt_pairs = get_sorted_matrix_on_weights(self.new_object_map, np.average(self.weight, axis=0), return_order=True)\n",
        "        elif (len(args)) == 1:\n",
        "            self.centers = pd.read_csv(os.path.join(args[0], \"centers.csv\")).to_numpy()\n",
        "            self.new_object_map = pd.read_csv(os.path.join(args[0], \"new_object_map.csv\")).to_numpy()\n",
        "            self.weight = pd.read_csv(os.path.join(args[0], \"weight.csv\")).to_numpy()\n",
        "            self.sub_wt_pairs = pd.read_csv(os.path.join(args[0], \"sub_wt_pairs.csv\")).to_numpy()\n",
        "        else:\n",
        "            raise Exception(\"Invalid object instantiation: parameters must be either <data_matrix:2D numpy>,<k:int> \"\n",
        "                            \"or <folder:string>.\")\n",
        "\n",
        "    def get_decomposition(self):\n",
        "        \"\"\"\n",
        "        Parameters:\n",
        "            X: ndarray of shape (num_objects, num_features)\n",
        "                Data matrix to be reduced\n",
        "\n",
        "        Returns:\n",
        "            Transforms and returns X in the latent semantic space and the latent semantics\n",
        "        \"\"\"\n",
        "        return self.centers\n",
        "\n",
        "    def get_latent_features(self):\n",
        "        \"\"\"\n",
        "        :return: centers\n",
        "        \"\"\"\n",
        "        return self.centers\n",
        "\n",
        "    def transform(self, data_matrix):\n",
        "        \"\"\"\n",
        "        :param data_matrix: matrix to transform (query_objects, num_features)\n",
        "        :return: pca transformed matrix\n",
        "        \"\"\"\n",
        "        new_map = np.zeros((len(data_matrix), len(self.centers)))\n",
        "        for j in range(len(self.centers)):\n",
        "            new_map[j] = manhattan_fn(data_matrix[:len(self.centers)][j], self.centers[j])\n",
        "        return new_map\n",
        "\n",
        "    def get_obj_weight_pairs(self):\n",
        "        \"\"\"\n",
        "        :return: objects\n",
        "        :return: weights\n",
        "        \"\"\"\n",
        "        return self.sub_wt_pairs\n",
        "\n",
        "    def save(self, folder):\n",
        "        \"\"\"\n",
        "        Save Kmeans to given folder\n",
        "        \"\"\"\n",
        "        pd.DataFrame(self.centers).to_csv(os.path.join(folder, \"centers.csv\"), index=False)\n",
        "        pd.DataFrame(self.new_object_map).to_csv(os.path.join(folder, \"new_object_map.csv\"), index=False)\n",
        "        pd.DataFrame(self.weight).to_csv(os.path.join(folder, \"weight.csv\"), index=False)\n",
        "        pd.DataFrame(self.sub_wt_pairs).to_csv(os.path.join(folder, \"sub_wt_pairs.csv\"), index=False)\n",
        "\n",
        "    def get_top_k_matches(self, k, xq):\n",
        "        return manhattan(self.new_object_map, k, self.transform([xq]))\n",
        "\n"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KJK56QNmL8uS"
      },
      "source": [
        "# Instantiate"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GLhr17SLL_SU"
      },
      "source": [
        "A = np.array([\n",
        "\t[1,2,3,4,5,6,7,8,9,10],\n",
        "\t[11,12,13,14,15,16,17,18,19,20],\n",
        "\t[21,22,23,24,25,26,27,28,29,30],\n",
        "\t[42,44,46,48,50,52,54,56,58,60]])"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fArXLAc6Lglx"
      },
      "source": [
        "# LDA class calls"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hpNgkzGHLkzP"
      },
      "source": [
        "lda_f = lda(A, 3)\n",
        "lda_f.save(\"\")\n",
        "lda_q.get_top_k_matches(2, A[0])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NiEemDpeNRtm"
      },
      "source": [
        "lda_f.get_obj_weight_pairs()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nxF6eCGmLlwi"
      },
      "source": [
        "lda_q = lda(\"\")\n",
        "lda_q.get_top_k_matches(2, A[0])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Oj7sh8GdLnJz"
      },
      "source": [
        "# PCA class calls"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yEjjhsg7LpNY"
      },
      "source": [
        "print(np.cov(A))\n",
        "pca = Pca(3,A)\n",
        "np.dot(pca.get_decomposition()[0], np.dot(pca.get_decomposition()[1],pca.get_decomposition()[2]))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2l4IKU9qLq9S"
      },
      "source": [
        "np.dot(*pca.get_latent_features()), pca.get_obj_weight_pairs()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1UfZ1zRFMrkE"
      },
      "source": [
        "pca.get_top_k_matches(2, A[0])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-dsvNyDULrby"
      },
      "source": [
        "# SVD calls"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B7H48Iz7LsQi"
      },
      "source": [
        "svd = Svd(A,3)\n",
        "np.dot(svd.get_decomposition()[0], np.dot(svd.get_decomposition()[1], svd.get_decomposition()[2]))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aTcq7G3OLssJ"
      },
      "source": [
        "np.dot(*svd.get_latent_features())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JxjSQ1lHMT8z"
      },
      "source": [
        "svd.get_top_k_matches(2, A[0])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "37UlBXW9LtBA"
      },
      "source": [
        "# Kmeans calls"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zUZavvKSLuiP"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tfU80wbkLv0O"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nzMygF06Lv-G"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TMNmhdZqLwOs"
      },
      "source": [
        "# Wrapper function"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 132
        },
        "id": "i-NPaCbTLxqJ",
        "outputId": "1317e49e-f829-4139-f064-3b85d580cc5c"
      },
      "source": [
        "def perform_dimensionality_reductions(matrix, k, technique):\n",
        "  if technique==\"pca\":\n",
        "    obj = Pca(k,matrix)\n",
        "  elif technique==\"svd\":\n",
        "    obj = Svd(k,matrix)\n",
        "  else:\n",
        "    obj = Lda(k,matrix)\n",
        "  obj.save(\"\")\n",
        "  return obj.get_obj_weight_pairs"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "error",
          "ename": "SyntaxError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-8-f28317250997>\"\u001b[0;36m, line \u001b[0;32m1\u001b[0m\n\u001b[0;31m    def perform_dimensionality_reductions():\u001b[0m\n\u001b[0m                                            ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m unexpected EOF while parsing\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6s1E2t6rY0QO"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}